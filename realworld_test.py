#!/usr/bin/env python3
"""
Real-world performance test for fansale.py vs fansale_v2.py
Runs each bot for a fixed time period and measures actual performance
"""

import time
import subprocess
import json
import os
import signal
import threading
from pathlib import Path
from datetime import datetime

class RealWorldTest:
    def __init__(self, test_duration=60):  # 60 seconds test for each version
        self.test_duration = test_duration
        self.results = {
            'test_date': datetime.now().isoformat(),
            'test_duration_seconds': test_duration,
            'v1': {},
            'v2': {}
        }
        
    def run_bot_test(self, script_name, version):
        """Run bot for test duration and collect stats"""
        print(f"\nü§ñ Running {script_name} for {self.test_duration} seconds...")
        
        # Clear stats before test
        stats_file = Path('fansale_stats.json')
        if stats_file.exists():
            stats_file.unlink()
            
        # Create input script to auto-configure bot
        auto_input = "1\n1\n3\n"  # 1 browser, 1 ticket, all Prato
        
        try:
            # Start the bot
            process = subprocess.Popen(
                [os.sys.executable, script_name],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
            
            # Send auto configuration
            process.stdin.write(auto_input)
            process.stdin.flush()
            
            # Wait for "Press Enter to START" and send enter
            time.sleep(5)  # Give time for browser to load
            process.stdin.write("\n")
            process.stdin.flush()
            
            # Let it run for test duration
            print(f"‚è±Ô∏è  Running test...")
            test_start = time.time()
            
            # Monitor for test duration
            while time.time() - test_start < self.test_duration:
                if process.poll() is not None:
                    print("‚ùå Bot crashed during test")
                    break
                time.sleep(1)
                
            # Stop the bot
            process.terminate()
            time.sleep(2)
            if process.poll() is None:
                process.kill()
                
            # Read the stats
            if stats_file.exists():
                with open(stats_file, 'r') as f:
                    stats = json.load(f)
                    
                # Calculate performance metrics
                total_checks = stats.get('total_checks', 0)
                checks_per_minute = (total_checks / self.test_duration) * 60 if self.test_duration > 0 else 0
                
                self.results[version] = {
                    'total_checks': total_checks,
                    'checks_per_minute': round(checks_per_minute, 1),
                    'blocks_encountered': stats.get('blocks_encountered', 0),
                    'unique_tickets_found': stats.get('unique_tickets_found', 0),
                    'raw_stats': stats
                }
                
                print(f"‚úÖ Completed: {total_checks} checks ({checks_per_minute:.1f}/min)")
                
            else:
                print("‚ùå No stats file found")
                self.results[version]['error'] = "No stats generated"
                
        except Exception as e:
            print(f"‚ùå Error during test: {e}")
            self.results[version]['error'] = str(e)
            
    def analyze_results(self):
        """Analyze and compare results"""
        print("\n\n" + "=" * 70)
        print("üèÜ REAL-WORLD PERFORMANCE COMPARISON")
        print("=" * 70)
        
        v1_data = self.results.get('v1', {})
        v2_data = self.results.get('v2', {})
        
        # Checks per minute comparison
        v1_cpm = v1_data.get('checks_per_minute', 0)
        v2_cpm = v2_data.get('checks_per_minute', 0)
        
        print(f"\nüìä Checks Per Minute:")
        print(f"   V1: {v1_cpm:.1f} checks/min")
        print(f"   V2: {v2_cpm:.1f} checks/min")
        
        if v1_cpm > 0 and v2_cpm > 0:
            diff = v2_cpm - v1_cpm
            pct = (diff / v1_cpm) * 100
            
            if diff < 0:
                print(f"   üìâ V2 is {abs(diff):.1f} checks/min slower ({abs(pct):.1f}%)")
                print(f"      This is expected due to human-like pauses")
            else:
                print(f"   üìà V2 is {diff:.1f} checks/min faster ({pct:.1f}%)")
                
        # Total checks comparison
        v1_total = v1_data.get('total_checks', 0)
        v2_total = v2_data.get('total_checks', 0)
        
        print(f"\nüîç Total Checks in {self.test_duration}s:")
        print(f"   V1: {v1_total} checks")
        print(f"   V2: {v2_total} checks")
        
        # Blocks encountered
        v1_blocks = v1_data.get('blocks_encountered', 0)
        v2_blocks = v2_data.get('blocks_encountered', 0)
        
        if v1_blocks > 0 or v2_blocks > 0:
            print(f"\nüö´ Blocks Encountered:")
            print(f"   V1: {v1_blocks}")
            print(f"   V2: {v2_blocks}")
            
        # Key differences
        print(f"\nüîë Key Differences:")
        print(f"   ‚Ä¢ V2 loads pages ~20-30% faster (no images)")
        print(f"   ‚Ä¢ V2 has staggered refresh (less detectable)")
        print(f"   ‚Ä¢ V2 simulates human behavior (5% distraction rate)")
        print(f"   ‚Ä¢ V2 staggers browser startup (3-7s delays)")
        
        # Winner determination
        print(f"\nüèÜ WINNER ANALYSIS:")
        
        # Speed consideration
        speed_penalty = abs(v2_cpm - v1_cpm) / v1_cpm * 100 if v1_cpm > 0 else 0
        
        if speed_penalty < 10:  # Less than 10% speed difference
            print(f"   ‚úÖ V2 WINS - Better anti-detection with minimal speed impact")
            print(f"   ‚Ä¢ Only {speed_penalty:.1f}% slower but much safer")
            print(f"   ‚Ä¢ Faster page loads compensate for pause patterns")
            print(f"   ‚Ä¢ Significantly lower ban risk")
        elif speed_penalty < 20:
            print(f"   ‚úÖ V2 RECOMMENDED - Good balance of speed and safety")
            print(f"   ‚Ä¢ {speed_penalty:.1f}% slower but worth it for safety")
            print(f"   ‚Ä¢ Better for long-term operation")
        else:
            print(f"   ‚ö†Ô∏è  SITUATIONAL - Choose based on priorities")
            print(f"   ‚Ä¢ V2 is {speed_penalty:.1f}% slower")
            print(f"   ‚Ä¢ Use V1 for speed, V2 for safety")
            
        print("\n" + "=" * 70)
        
    def run(self):
        """Run the complete test"""
        print("üöÄ Starting Real-World Performance Test")
        print(f"‚è±Ô∏è  Each version will run for {self.test_duration} seconds")
        print("=" * 70)
        
        # Backup existing stats
        stats_file = Path('fansale_stats.json')
        if stats_file.exists():
            backup = f'fansale_stats_backup_{int(time.time())}.json'
            stats_file.rename(backup)
            print(f"üì¶ Backed up existing stats to {backup}")
            
        # Test V1
        print(f"\nüìã Testing V1 (Original)")
        self.run_bot_test('fansale.py', 'v1')
        
        # Brief pause between tests
        print("\n‚è∏Ô∏è  Pausing before next test...")
        time.sleep(5)
        
        # Test V2
        print(f"\nüìã Testing V2 (Enhanced)")
        self.run_bot_test('fansale_v2.py', 'v2')
        
        # Analyze results
        self.analyze_results()
        
        # Save detailed results
        with open('realworld_test_results.json', 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"\nüíæ Detailed results saved to realworld_test_results.json")

if __name__ == "__main__":
    # Run a 60-second test for each version
    tester = RealWorldTest(test_duration=60)
    tester.run()
